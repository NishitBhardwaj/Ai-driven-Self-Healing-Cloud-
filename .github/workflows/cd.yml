name: CD Pipeline

on:
  push:
    branches: [ main ]
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      aws_region:
        description: 'AWS Region (overrides secret)'
        required: false
        default: ''
        type: choice
        options:
          - us-east-1
          - us-west-2
          - ap-south-1
          - ca-central-1
          - eu-west-1
          - eu-central-1

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository_owner }}/ai-cloud
  KUBERNETES_NAMESPACE: ai-cloud-${{ github.event.inputs.environment || 'staging' }}

jobs:
  # Deploy to Kubernetes
  deploy:
    name: Deploy to Kubernetes
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'staging' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Debug EKS vars
        run: |
          CLUSTER_NAME='${{ secrets.EKS_CLUSTER_NAME }}'
          REGION='${{ secrets.AWS_REGION }}'
          echo "Cluster name -> >$CLUSTER_NAME<"
          echo "Region       -> >$REGION<"
          echo "Name length  -> ${#CLUSTER_NAME}"
          if [ -z "$CLUSTER_NAME" ]; then
            echo "ERROR: EKS_CLUSTER_NAME is empty!"
            exit 1
          fi
          if [ -z "$REGION" ]; then
            echo "ERROR: AWS_REGION is empty!"
            exit 1
          fi
          echo "EKS variables are set correctly"

      - name: Configure kubectl for EKS
        run: |
          set -x
          aws eks update-kubeconfig \
            --name "${{ secrets.EKS_CLUSTER_NAME }}" \
            --region "${{ secrets.AWS_REGION }}"


      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Create namespace
        run: |
          kubectl create namespace ${{ env.KUBERNETES_NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy with Helm
        run: |
          helm upgrade --install ai-cloud \
            kubernetes/helm/ai-cloud \
            --namespace ${{ env.KUBERNETES_NAMESPACE }} \
            --set image.registry=${{ env.REGISTRY }} \
            --set image.prefix=${{ env.IMAGE_PREFIX }} \
            --set image.tag=${{ github.sha }} \
            --set environment=${{ github.event.inputs.environment || 'staging' }} \
            --wait \
            --timeout 10m

      - name: Verify deployment
        run: |
          kubectl rollout status deployment/ai-cloud-self-healing -n ${{ env.KUBERNETES_NAMESPACE }} --timeout=5m
          kubectl rollout status deployment/ai-cloud-scaling -n ${{ env.KUBERNETES_NAMESPACE }} --timeout=5m
          kubectl rollout status deployment/ai-cloud-task-solving -n ${{ env.KUBERNETES_NAMESPACE }} --timeout=5m

      - name: Run smoke tests
        run: |
          # Wait for services to be ready
          sleep 30
          
          # Test health endpoints
          kubectl run -it --rm test-client --image=curlimages/curl --restart=Never -- \
            curl -f http://ai-cloud-self-healing:8080/health || exit 1
          
          kubectl run -it --rm test-client --image=curlimages/curl --restart=Never -- \
            curl -f http://ai-cloud-scaling:8080/health || exit 1

  # Rollback on failure
  rollback:
    name: Rollback on Failure
    runs-on: ubuntu-latest
    needs: [deploy]
    if: failure()
    env:
      KUBERNETES_NAMESPACE: ai-cloud-${{ github.event.inputs.environment || 'staging' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Validate AWS Region
        run: |
          if [ -z "${{ secrets.AWS_REGION }}" ]; then
            echo "ERROR: AWS_REGION is not set!"
            exit 1
          fi
          echo "Using AWS Region: ${{ secrets.AWS_REGION }}"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Configure kubectl for EKS
        run: |
          set -x
          if [ -z "${{ secrets.EKS_CLUSTER_NAME }}" ]; then
            echo "ERROR: EKS_CLUSTER_NAME secret is not set!"
            exit 1
          fi
          echo "Configuring kubectl for EKS cluster: ${{ secrets.EKS_CLUSTER_NAME }}"
          echo "AWS Region: ${{ secrets.AWS_REGION }}"
          aws eks update-kubeconfig --name "${{ secrets.EKS_CLUSTER_NAME }}" --region "${{ secrets.AWS_REGION }}"
          
          # Verify kubectl is configured correctly
          echo "Verifying kubectl configuration..."
          kubectl cluster-info || echo "Warning: kubectl cluster-info failed, but continuing..."

      - name: Rollback Helm release
        run: |
          helm rollback ai-cloud -n ${{ env.KUBERNETES_NAMESPACE }} || echo "Rollback failed or no previous release"

      - name: Notify team
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: 'Deployment failed and was rolled back'
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        continue-on-error: true

